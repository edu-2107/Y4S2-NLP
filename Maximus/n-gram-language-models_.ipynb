{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1GaX95nnkQbu"
   },
   "source": [
    "Notebook prepared by Henrique Lopes Cardoso (hlc@fe.up.pt), based on [A Comprehensive Guide to Build your own Language Model in Python](https://www.analyticsvidhya.com/blog/2019/08/comprehensive-guide-language-model-nlp-python-code/) by Mohd Sanad Zaki Rizvi, and on the [Infini-gram](https://infini-gram.io/) documentation.\n",
    "\n",
    "# N-GRAM LANGUAGE MODELS\n",
    "\n",
    "N-gram language models are based on computing probabilities for the occurrence of each word given *n-1* previous words.\n",
    "\n",
    "To \"train\" such models, we will make use of the [Reuters](https://www.nltk.org/book/ch02.html) corpus, which contains 10,788 news documents in a total of 1.3 million words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 2129,
     "status": "ok",
     "timestamp": 1645451765839,
     "user": {
      "displayName": "Henrique Lopes Cardoso",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiDm6eMLxxGrmSt7hX7Fe3fSYQXSU2koR-YesCvdw=s64",
      "userId": "16701394035750291027"
     },
     "user_tz": 0
    },
    "id": "HqoClhOHkQb2"
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import reuters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check the number of sentences there are in the corpus. Each sentence is a list of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package reuters to\n",
      "[nltk_data]     /Users/davideteixeira/nltk_data...\n",
      "[nltk_data]   Package reuters is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54716\n",
      "['ASIAN', 'EXPORTERS', 'FEAR', 'DAMAGE', 'FROM', 'U', '.', 'S', '.-', 'JAPAN', 'RIFT', 'Mounting', 'trade', 'friction', 'between', 'the', 'U', '.', 'S', '.', 'And', 'Japan', 'has', 'raised', 'fears', 'among', 'many', 'of', 'Asia', \"'\", 's', 'exporting', 'nations', 'that', 'the', 'row', 'could', 'inflict', 'far', '-', 'reaching', 'economic', 'damage', ',', 'businessmen', 'and', 'officials', 'said', '.']\n",
      "ASIAN EXPORTERS FEAR DAMAGE FROM U . S .- JAPAN RIFT Mounting trade friction between the U . S . And Japan has raised fears among many of Asia ' s exporting nations that the row could inflict far - reaching economic damage , businessmen and officials said . "
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('reuters')\n",
    "\n",
    "print(len(reuters.sents()))\n",
    "\n",
    "print(reuters.sents()[0])\n",
    "for w in reuters.sents()[0]:\n",
    "    print(w, end=' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xSbkDouwkQb4"
   },
   "source": [
    "## Unigram model\n",
    "\n",
    "For starters, let's build a unigram language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 25,
     "status": "error",
     "timestamp": 1645451769412,
     "user": {
      "displayName": "Henrique Lopes Cardoso",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiDm6eMLxxGrmSt7hX7Fe3fSYQXSU2koR-YesCvdw=s64",
      "userId": "16701394035750291027"
     },
     "user_tz": 0
    },
    "id": "VaLWQjxmkQb5",
    "outputId": "67ec3ca8-4982-46c7-a1a2-3cba411b8956"
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "# Create a placeholder for the model\n",
    "uni_model = defaultdict(int)\n",
    "\n",
    "# Count the frequency of each token\n",
    "for sentence in reuters.sents():\n",
    "    for w in sentence:\n",
    "        uni_model[w] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the counts, we need to transform them into probabilities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_count = float(sum(uni_model.values()))\n",
    "for w in uni_model:\n",
    "    uni_model[w] /= total_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Likely words\n",
    "\n",
    "How likely is the word 'the'?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58251.0\n"
     ]
    }
   ],
   "source": [
    "# your code here\n",
    "print(uni_model['the'] * total_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the most likely word in the corpus?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KRN 1.0\n"
     ]
    }
   ],
   "source": [
    "# your code here\n",
    "max = 0\n",
    "max_key = \"\"\n",
    "for key in uni_model:\n",
    "    if uni_model[key] > max:\n",
    "        max = uni_model[key]\n",
    "        max_key = key\n",
    "        \n",
    "print(key, uni_model[key] * total_count)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generating text\n",
    "\n",
    "Based on this unigram language model, we can try generating some text. It will not be pretty, though..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "-IsAHfWikQb6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adjustment greatest futures Finance guesses January vs , 27 466 . mln , of has - month 900 over adverse - Therapies at MOORE default in dlrs & new for noted higher AFTERNOON tube the U said for vs products mln noting two studying SALE February destined 000 Energy 000 Corp of to of the , Santa to of not venture a Liro QTR from said a one / Avg the QTR equalling Dart cts currency than dollar . quarter sells on dlrs Congress of Net each ROWTON from ^ processing . issue candidate party the of difficult to Spie\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# number of words to generate\n",
    "total_words = 100\n",
    "text = []\n",
    "\n",
    "for i in range(total_words):\n",
    "    # select a random probability threshold\n",
    "    r = random.random()\n",
    "\n",
    "    # select word above the probability threshold\n",
    "    accumulator = .0\n",
    "    for word in uni_model.keys():\n",
    "        accumulator += uni_model[word]\n",
    "        if accumulator >= r:\n",
    "            text.append(word)\n",
    "            break\n",
    "\n",
    "print (' '.join([t for t in text]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iNyUfDqNkQb7"
   },
   "source": [
    "## Bigram model\n",
    "\n",
    "In a bigram model, we'll compute the probability of each word given the previous word as context. To obtain bigrams, we can use NLTK's [bigrams](https://www.nltk.org/_modules/nltk/util.html#bigrams). When doing so, we can pad the input left and right and define our own sequence start and sequence end symbols.\n",
    "\n",
    "We first need to obtain the counts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "HOIR6wVBkQb7"
   },
   "outputs": [],
   "source": [
    "from nltk import bigrams\n",
    "\n",
    "# Create a placeholder for the model\n",
    "bi_model = defaultdict(lambda: defaultdict(lambda: 0))\n",
    "\n",
    "# Count the frequency of each bigram\n",
    "for sentence in reuters.sents():\n",
    "    for w1, w2 in bigrams(sentence, pad_right=True, pad_left=True, left_pad_symbol='<s>', right_pad_symbol='</s>'):\n",
    "        bi_model[w1][w2] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, we need to transform counts into probabilities. For that, we divide each count by the total number of occurrences of the first word in the bigram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Likely pairs\n",
    "\n",
    "What are the probabilities of each word following 'today'?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1jvD75QekQb9"
   },
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are the probabilities for sentence-starting words? What do most of them have in common? (Hint: check the *left_pad_symbol* defined above for collecting bigrams.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1jvD75QekQb9"
   },
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generating text\n",
    "\n",
    "Now that we have a bigram model, we can generate text based on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xRXR0uxHkQb9"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# sequence start symbol\n",
    "text = [\"<s>\"]\n",
    "\n",
    "# generate text until we find the end of sequence symbol\n",
    "while text[-1] != \"</s>\":\n",
    "    # select a random probability threshold\n",
    "    r = random.random()\n",
    "    \n",
    "    # select word above the probability threshold, conditioned to the previous word text[-1]\n",
    "    # your code here\n",
    "    \n",
    "\n",
    "print (' '.join([t for t in text]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NAjueaY4kQb-"
   },
   "source": [
    "## Trigram model\n",
    "\n",
    "In a trigram model, we'll compute the probability of each word given the previous two words as context. To obtain trigrams, we can use NLTK's [trigrams](https://www.nltk.org/_modules/nltk/util.html#trigrams)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UFpa4uXHkQb_"
   },
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Likely triplets\n",
    "\n",
    "What are the most likely words following \"today the\"?\n",
    "What about \"England has\"?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Vt-tvT57kQcA"
   },
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generating text\n",
    "\n",
    "Create your text generator based on the trigram model. Does the generated text start to feel a bit more sound?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YV2b_wVfkQcB"
   },
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ngWW-8YXkQcC"
   },
   "source": [
    "## N-gram models\n",
    "\n",
    "For larger *n*, we can use NLTK's [n-grams](https://www.nltk.org/_modules/nltk/util.html#ngrams), which allows us to choose an arbitrary *n*.\n",
    "\n",
    "Create your own 4-gram model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tDB-aucOkQcC"
   },
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Likely tuples\n",
    "\n",
    "Check the most likely words following \"today the public\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HkkHxPTTkQcD"
   },
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generating text\n",
    "\n",
    "Create your text generator based on the 4-gram model. Even better, uh?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OU3XXz10kQcD"
   },
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prompting the model\n",
    "\n",
    "Ask the user for some input as a prompt, and make your model continue the input text!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perplexity\n",
    "\n",
    "Now that you have several language models based on different n-gram sizes, you can compare them in terms of perplexity regarding some provided text. Before you do that, however, you might want to apply smoothing to your language models, if you haven't done so already. This will ensure that no sequence gets a probability of zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the unigram model, we first get the probability of a sequence by multiplying the probability of each token, and then apply the perplexity formula."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "\n",
    "text = input(\"Sentence: \")\n",
    "tokens = word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uni_prob = 1\n",
    "for token in tokens:\n",
    "    uni_prob *= uni_model[token]\n",
    "\n",
    "uni_perp = pow(uni_prob, -1/len(tokens))\n",
    "\n",
    "print(\"Unigram probability: \", uni_prob, \"\\nUnigram perplexity: \", uni_perp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do the same for the larger n language models. Don't forget to pad the sentence for each case.\n",
    "What do you observe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Infini-gram\n",
    "\n",
    "[∞-gram](https://infini-gram.io/) is a Language Model with backoff that can compute unbounded n-gram counts in a very efficient way. Instead of pre-computing n-gram count tables (which would be very expensive), the infini-gram engine can compute ∞-gram (as well as n-gram with arbitrary n) probabilities with millisecond-level latency.\n",
    "- You can read more about infini-gram in [Infini-gram: Scaling Unbounded n-gram Language Models to a Trillion Tokens](https://arxiv.org/abs/2401.17377).\n",
    "- And you can try it out directly in the Hugging Face [infini-gram](https://huggingface.co/spaces/liujch1998/infini-gram) web interface.\n",
    "\n",
    "Infini-gram includes indexes on several corpora, which can be queried for the purposes below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Infini-gram can be used via its [API endpoint](https://infini-gram.io/api_doc) or via its [Python package](https://infini-gram.io/pkg_doc) (available for Linux distributions).\n",
    "\n",
    "Infini-gram includes methods for:\n",
    "- Counting the number of times an n-gram appears in the corpus.\n",
    "- Computing the n-gram probability of the last token conditioned on the previous tokens.\n",
    "- Computing the next-token distribution of an (n-1)-gram.\n",
    "- Computing the ∞-gram probability of the last token conditioned on the previous tokens, with backoff.\n",
    "- Computing the next-token distribution of an ∞-gram, with backoff.\n",
    "- Search for documents containing n-gram(s).\n",
    "\n",
    "Let's try out each of these via the [API endpoint](https://infini-gram.io/api_doc)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Count an n-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "index = 'v4_rpj_llama_s4'\n",
    "ngram = 'University of Porto'\n",
    "\n",
    "payload = {\n",
    "    'index': index,\n",
    "    'query_type': 'count',\n",
    "    'query': ngram,\n",
    "}\n",
    "result = requests.post('https://api.infini-gram.io/', json=payload).json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The ngram \", result.get('tokens'), \" appears\", result.get('count'), \"times in\", index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute the probability of the last token in an n-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngram = 'University of Porto is the'\n",
    "\n",
    "payload = {\n",
    "    'index': index,\n",
    "    'query_type': 'prob',\n",
    "    'query': ngram,\n",
    "}\n",
    "result = requests.post('https://api.infini-gram.io/', json=payload).json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The ngram\", result.get('tokens')[0:-1], \"appears\", result.get('prompt_cnt'), \"times, \\nfollowed by\", result.get('tokens')[-1], \"in\", result.get('cont_cnt'), \"of those, \\nfor a probability of\", result.get('prob'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute the next-token distribution of an (n-1)-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "payload = {\n",
    "    'index': index,\n",
    "    'query_type': 'ntd',\n",
    "    'query': ngram,\n",
    "}\n",
    "result = requests.post('https://api.infini-gram.io/', json=payload).json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in result.get('result_by_token_id').values():\n",
    "    print(f\"{t.get('token'):15}\", \"appears\", f\"{t.get('cont_cnt'):3}\", \"times, for a probability of\", t.get('prob'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute the ∞-gram probability of the last token\n",
    "\n",
    "Comparing n-gram with ∞-gram probabilities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngram = 'since University of Porto is the'\n",
    "\n",
    "payload = {\n",
    "    'index': index,\n",
    "    'query_type': 'prob',\n",
    "    'query': ngram,\n",
    "}\n",
    "result = requests.post('https://api.infini-gram.io/', json=payload).json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result.get('prob'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "payload = {\n",
    "    'index': index,\n",
    "    'query_type': 'infgram_prob',\n",
    "    'query': ngram,\n",
    "}\n",
    "result = requests.post('https://api.infini-gram.io/', json=payload).json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The longest found suffix is '\", result.get('longest_suffix'), \"' and appears\", result.get('prompt_cnt'), \"times, \\nfollowed by\", result.get('tokens')[-1], \"in\", result.get('cont_cnt'), \"of those, \\nfor a probability of\", result.get('prob'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute the ∞-gram next-token distribution\n",
    "\n",
    "Comparing n-gram with ∞-gram next-token distributions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngram = 'since University of Porto is the'\n",
    "\n",
    "payload = {\n",
    "    'index': index,\n",
    "    'query_type': 'ntd',\n",
    "    'query': ngram,\n",
    "}\n",
    "result = requests.post('https://api.infini-gram.io/', json=payload).json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.get('result_by_token_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "payload = {\n",
    "    'index': index,\n",
    "    'query_type': 'infgram_ntd',\n",
    "    'query': ngram,\n",
    "}\n",
    "result = requests.post('https://api.infini-gram.io/', json=payload).json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The longest found suffix is '\", result.get('longest_suffix'), \"' and appears\", result.get('prompt_cnt'), \"times\")\n",
    "for t in result.get('result_by_token_id').values():\n",
    "    print(f\"{t.get('token'):15}\", \"appears\", f\"{t.get('cont_cnt'):3}\", \"times, for a probability of\", t.get('prob'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Search for documents containing n-gram(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngram = 'University of Porto'\n",
    "\n",
    "payload = {\n",
    "    'index': index,\n",
    "    'query_type': 'search_docs',\n",
    "    'query': ngram,\n",
    "    'maxnum': 3,\n",
    "}\n",
    "result = requests.post('https://api.infini-gram.io/', json=payload).json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(result.get('message'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in result.get('documents'):\n",
    "    print(d.get('doc_ix'))\n",
    "    for s in d.get('spans'):\n",
    "        print(s[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generating text\n",
    "\n",
    "By exploring ∞-gram next-token distributions, we can create a text generator that takes into account these probabilities. Can you do it?\n",
    "\n",
    "(Note that by relying on the API endpoint we end up needing to make successive calls to the API, which is pretty slow.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "language-models.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
